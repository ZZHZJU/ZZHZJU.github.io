# 监督学习

## 简介

监督学习是一种机器学习方法，它通过学习已知输入数据和相应标签之间的映射关系，来预测新数据的标签。常见的监督学习算法包括线性回归、逻辑回归、支持向量机等。

## 常见算法

### 线性回归

**线性回归**是一种用于预测连续变量的模型。其核心思想是找到一条最佳拟合直线，使得训练数据点与这条直线之间的误差最小。

#### 数学表达

假设我们有 $N$个训练样本和它们的标签 $\{(X_1, y_1), (X_2, y_2), \dots, (X_N, y_N)\}$，其中：
- $X_i = [X_{i1}, X_{i2}]^T$表示第 $i$个样本的特征向量
- $y_i \in \{+1, -1\}$表示第 $i$个样本的标签

当数据在 $N$维特征空间是线性可分的时，存在一个 $( \omega_1, \omega_2, b )$使得对于所有样本 $i$满足以下条件：
- 若 $y_i = +1$，则 $\omega_1 X_{i1} + \omega_2 X_{i2} + b > 0$
- 若 $y_i = -1$，则 $\omega_1 X_{i1} + \omega_2 X_{i2} + b < 0$

可以将上述条件简化为向量形式：
$y_i (\omega^T X_i + b) > 0$

### 线性回归代价函数

在线性回归中，常用的代价函数是**最小平方误差（MSE）**：
$\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - f(x_i, w))^2$

另一种常见的代价函数是**残差平方和（RSS）**：
$J_n(\alpha) = \sum_{i=1}^n (y_i - f(x_i, w))^2 = (y - X^T w)^T (y - X^T w)$

### 线性回归模型

#### 普通最小二乘法

普通线性回归模型的权重计算公式为：
$w = (XX^T)^{-1} Xy$

当样本数量 $n$小于特征数 $d$时，矩阵 $XX^T$不满秩，无法求逆矩阵，导致线性回归模型存在多个解。

#### 岭回归

为了避免线性回归中的过拟合问题，引入了正则化方法，岭回归的优化目标为：
$w^* = (XX^T + \lambda I)^{-1} Xy$
其中 $\lambda$是正则化参数。

### 逻辑回归

逻辑回归用于**二分类**问题，注意这是一个分类算法，输出值为 $y \in \{0, 1\}$，其中 0 表示负向类，1 表示正向类。通过一个 Sigmoid 函数将输入映射到 $(0, 1)$之间，从而估计样本属于某个类别的概率。

常用的 Sigmoid 函数为：
$y = \frac{1}{1 + e^{-z}}$

### 决策树

**决策树**是一种用于分类和回归的树形结构模型。通过对数据集的特征进行递归划分，形成树状结构，最终根据特征的值来进行预测或分类。

#### 结构

- **根节点（Root Node）**：表示整个数据集，决策从这里开始。
- **内部节点（Internal Nodes）**：表示一个特征，基于该特征的条件分割数据集。
- **叶节点（Leaf Nodes）**：表示最终的决策输出。

#### 优化方法

- **剪枝（Pruning）**：通过减少树的复杂度来避免过拟合。
- **集成方法**：如随机森林（Random Forest），通过多个决策树的组合提高模型的准确性。

### 支持向量机

支持向量机通过最大化分类间隔（Margin）来实现分类任务。支持向量机的目标是找到一个最佳超平面，使得超平面两侧的间隔最大。

#### 数学描述

目标是最小化：
$\frac{1}{2} \|\omega\|^2$
并满足约束条件：
$y_i (\omega^T x_i + b) \geq 1$

在处理线性不可分问题时，可以引入松弛变量 $\delta_i$和核函数 $K(X_1, X_2)$以升维处理问题。

### k近邻算法

**KNN算法**是一种分类算法，通过找到与测试样本最接近的 $k$个训练样本，并根据这些样本的标签来预测测试样本的类别。

#### 距离度量

常用的距离度量包括：
- 曼哈顿距离 (p=1)
- 欧几里得距离 (p=2)
- 切比雪夫距离 (p=$\infty$)

### 感知机

**感知机**是一种简单的二分类模型，通过线性分类器对数据进行分类。感知机模型仅能处理线性可分的数据。

#### 训练过程

感知机训练通过反复调整权重，使得模型能够正确分类给定数据。在线性可分的情况下，感知机算法必然收敛。

#### 局限性

感知机无法处理线性不可分的数据，如 XOR 问题，并且其效率不高。