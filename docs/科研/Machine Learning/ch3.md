# 无监督学习

## 聚类

### 基本概念

**聚类**是一种无监督学习方法，目的是在给定的无标签数据集中，根据特征的相似度或距离，将数据对象划分为若干个类。每个类中的数据对象在某种意义上比它们与其他类中的对象更相似。

### 相似度与距离

- **闵可夫斯基距离 (Minkowski Distance)**：
  $d_{ij} = \left(\sum_{i=1}^m |x_{ik} - x_{jk}|^p\right)^{\frac{1}{p}}$
  闵可夫斯基距离是欧氏距离和曼哈顿距离的推广，通过参数 $p$ 来调整距离的计算方式。

- **马哈拉诺比斯距离 (Mahalanobis Distance)**：
  假设样本矩阵 $X$ 的协方差矩阵为 $S$，则马哈拉诺比斯距离为：
  $d_{ij} = \sqrt{(x_i - x_j)^T S^{-1} (x_i - x_j)}$
  马哈拉诺比斯距离考虑了各特征之间的相关性，距离值越大，相关性越小。

- **余弦相似度 (Cosine Similarity)**：
  $\cos(\theta) = \frac{x^T y}{|x| \cdot |y|} = \frac{\sum_{k=1}^m x_{ki} x_{kj}}{\sqrt{\sum_{k=1}^m x_{ki}^2} \sqrt{\sum_{k=1}^m x_{kj}^2}}$
  余弦相似度衡量两个向量之间的夹角，值越接近 1 表示两个向量越相似。

- **相关系数 (Correlation Coefficient)**：
  $r_{ij} = \frac{\sum_{k=1}^m (x_{ki} - \overline{x_i})(x_{kj} - \overline{x_j})}{\sqrt{\sum_{k=1}^m (x_{ki} - \overline{x_i})^2} \sqrt{\sum_{k=1}^m (x_{kj} - \overline{x_j})^2}}$
  相关系数衡量两个变量之间的线性相关性，值的范围在 -1 到 1 之间。

### K-means 算法

#### 描述

**K-均值聚类**是一种常用的基于划分的聚类算法。它将样本集合划分为 $k$ 个子集，每个样本只属于一个类。算法的基本思想是：对每个数据点，根据其与 $k$ 个中心点的距离，将其分配给最近的中心点，并根据分配结果重新计算每个类的中心点，反复迭代直到收敛。

#### 伪代码

```python
Repeat {
    for i = 1 to m:
        c(i) := index(from 1 to k) of cluster centroid closest to x(i)
    for k = 1 to K:
        uk := average(mean) of points assigned to cluster k
}
```

该算法分为两个主要步骤：
1. **赋值步骤**：对于每个样本 $i$，计算其所属的类，即找到与其距离最近的中心点。
2. **聚类中心移动步骤**：对于每个类 $k$，重新计算该类的中心，即计算与该类关联的所有样本的均值。

### 最大期望算法

**最大期望算法**（EM）被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计

## 降维

### 为什么需要降维

- **数据压缩 (Data Compression)**：降低维度可以减少数据存储和处理的空间需求，同时加速算法的计算过程。
- **可视化 (Visualization)**：降维可以将高维数据投射到低维空间，帮助我们更直观地理解数据分布，尤其是通过将数据降至二维或三维来进行可视化展示。

### 主成分分析

#### 描述

**主成分分析 (Principal Component Analysis, PCA)** 是一种常见的降维方法。PCA 的目标是找到一个方向向量，使得所有数据投影到该向量上的平均均方误差尽可能小。这个方向向量是一个过原点的向量，投影误差是从特征向量到该方向向量做垂线的长度。PCA 通过线性变换找到数据在新空间的主轴，并利用这些主轴来进行数据的降维处理。

### 线性判别分析

#### 描述

**线性判别分析 (Linear Discriminant Analysis, LDA)** 是另一种经典的降维算法，也是线性分类方法。LDA 的基本思想是将样本投影到一条直线上，使得同类样本点的投影尽可能接近，而不同类样本点的投影尽可能远离。对于测试集数据，通过投影到该直线上，可以直接判断样本所属的类别。LDA 适用于带有标签的分类问题，并在降维的同时保留了类间的差异性。